{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 009.mp4 and 009.csv\n",
      "Video: train1/009.mp4, Frames in Video: 6745, Gesture Values in CSV: 6745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain1\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your training folder path\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Process all videos and CSVs\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m sequences, labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_videos_and_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Normalize features\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sequences\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Check if any valid sequences were found\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 127\u001b[0m, in \u001b[0;36mprocess_all_videos_and_csvs\u001b[0;34m(folder_path, fps)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_path):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(csv_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m     video_features, video_labels \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Skip if the size does not match\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m video_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m video_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path, csv_path, fps)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Extract pose features from the frame every 0.066 seconds (15 FPS)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mround\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS) \u001b[38;5;241m*\u001b[39m frame_rate) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_pose_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features:\n\u001b[1;32m    104\u001b[0m         features_list\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mextract_pose_features\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     34\u001b[0m inp \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(frame, \u001b[38;5;241m1.0\u001b[39m, (inWidth, inHeight), (\u001b[38;5;241m127.5\u001b[39m, \u001b[38;5;241m127.5\u001b[39m, \u001b[38;5;241m127.5\u001b[39m), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(inp)\n\u001b[0;32m---> 36\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, :\u001b[38;5;241m19\u001b[39m, :, :]\n\u001b[1;32m     39\u001b[0m points \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# COCO body parts and pose pairs\n",
    "BODY_PARTS = {\"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "              \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "              \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "              \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18}\n",
    "\n",
    "# Load pre-trained pose estimation model\n",
    "net = cv2.dnn.readNetFromTensorflow(\"graph_opt.pb\")\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(a, b):\n",
    "    return math.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
    "\n",
    "# Function to extract pose features from a frame\n",
    "def extract_pose_features(frame):\n",
    "    inWidth = 368\n",
    "    inHeight = 368\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    inp = cv2.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False)\n",
    "    net.setInput(inp)\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]\n",
    "\n",
    "    points = []\n",
    "    threshold = 0.1\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        heatMap = out[0, i, :, :]\n",
    "        _, conf, _, point = cv2.minMaxLoc(heatMap)\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "        if conf > threshold:\n",
    "            points.append((int(x), int(y)))\n",
    "        else:\n",
    "            points.append(None)\n",
    "\n",
    "    # Define features: distances between keypoints\n",
    "    keypoint_pairs = [\n",
    "        (\"RWrist\", \"LWrist\"), (\"RWrist\", \"Neck\"), (\"LWrist\", \"Neck\"),\n",
    "        (\"Neck\", \"Nose\"), (\"Nose\", \"REye\"), (\"Nose\", \"LEye\"),\n",
    "        (\"RHip\", \"RKnee\"), (\"RKnee\", \"RAnkle\"), (\"LHip\", \"LKnee\"), (\"LKnee\", \"LAnkle\")\n",
    "    ]\n",
    "\n",
    "    features = []\n",
    "    for kp1, kp2 in keypoint_pairs:\n",
    "        idx1, idx2 = BODY_PARTS[kp1], BODY_PARTS[kp2]\n",
    "        if points[idx1] and points[idx2]:\n",
    "            distance = calculate_distance(points[idx1], points[idx2])\n",
    "            features.append(distance)\n",
    "        else:\n",
    "            features.append(0)  # Use 0 if keypoints are not found\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to process a single video and corresponding CSV\n",
    "def process_video(video_path, csv_path, fps=15):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features_list = []\n",
    "    \n",
    "    # Load the CSV gesture values without headers\n",
    "    gesture_df = pd.read_csv(csv_path, header=None)\n",
    "    \n",
    "    # Flatten the dataframe columns to create a single sequence of gesture values\n",
    "    gesture_values = gesture_df.values.flatten()  \n",
    "    \n",
    "    frame_num = 0\n",
    "    frame_rate = 1 / fps \n",
    "    num_frames = len(gesture_values)  # The number of frames to process should match the number of gesture values\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "  \n",
    "    print(f\"Video: {video_path}, Frames in Video: {total_frames}, Gesture Values in CSV: {num_frames}\")\n",
    "    \n",
    "    # Ignore if the number of frames and number of gesture values do not match\n",
    "    if total_frames != num_frames:\n",
    "        print(f\"Skipping {video_path} due to mismatch in number of frames and gesture values.\")\n",
    "        cap.release()\n",
    "        return None, None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Extract pose features from the frame every 15 FPS\n",
    "        if frame_num % round(cap.get(cv2.CAP_PROP_FPS) * frame_rate) == 0:\n",
    "            features = extract_pose_features(frame)\n",
    "            if features:\n",
    "                features_list.append(features)\n",
    "        \n",
    "        frame_num += 1\n",
    "        \n",
    "        # Stop if we have extracted enough frames matching the gesture values\n",
    "        if len(features_list) >= num_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(features_list), gesture_values[:len(features_list)]\n",
    "\n",
    "# Function to process all videos and CSV files in a folder\n",
    "def process_all_videos_and_csvs(folder_path, fps=15):\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.mp4'):\n",
    "            video_path = os.path.join(folder_path, file_name)\n",
    "            csv_path = video_path.replace('.mp4', '.csv')\n",
    "\n",
    "            if os.path.exists(csv_path):\n",
    "                print(f\"Processing: {file_name} and {os.path.basename(csv_path)}\")\n",
    "                video_features, video_labels = process_video(video_path, csv_path, fps)\n",
    "\n",
    "                # Skip if the size does not match\n",
    "                if video_features is None or video_labels is None:\n",
    "                    continue\n",
    "\n",
    "                # Sequence length for LSTM\n",
    "                sequence_length = 10\n",
    "\n",
    "                # Prepare sequences from the video features\n",
    "                for i in range(len(video_features) - sequence_length):\n",
    "                    all_sequences.append(video_features[i:i + sequence_length])\n",
    "                    all_labels.append(video_labels[i + sequence_length - 1])  # Label from the last frame in the sequence\n",
    "\n",
    "    return np.array(all_sequences), np.array(all_labels)\n",
    "\n",
    "# Path to the folder containing videos and CSV files\n",
    "folder_path = 'train1'  \n",
    "\n",
    "# Process all videos and CSVs\n",
    "sequences, labels = process_all_videos_and_csvs(folder_path)\n",
    "\n",
    "# Normalize features\n",
    "if sequences.shape[0] > 0:  \n",
    "    scaler = StandardScaler()\n",
    "    num_features = sequences.shape[2]\n",
    "    sequences = scaler.fit_transform(sequences.reshape(-1, num_features)).reshape(-1, sequences.shape[1], num_features)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save('gesture_recognition_model.keras')  \n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Predict the labels for the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  \n",
    "\n",
    "    # Calculate the accuracy score\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Generate the classification report\n",
    "    class_report = classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "else:\n",
    "    print(\"No valid sequences were found during processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom gesture labels\n",
    "gesture_labels = [\"Idle\", \"Stop\", \"Pass\", \"Turn left\", \"Left wait\", \"Turn right\", \"Change Lane\", \"Slow down\", \"Get off\"]\n",
    "\n",
    "# Manually assign these labels to the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = np.array(gesture_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the class with highest probability\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate the confusion matrix (raw)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Normalize the confusion matrix to display values between 0 and 1\n",
    "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the normalized confusion matrix (0-1 values)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=gesture_labels, yticklabels=gesture_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Generate the classification report\n",
    "try:\n",
    "    class_report = classification_report(y_test, y_pred_classes, target_names=gesture_labels)\n",
    "    print(\"Classification Report:\\n\", class_report)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}. Check that `y_test` and `gesture_labels` have matching labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "plot_training_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
